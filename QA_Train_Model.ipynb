{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess the SQUAD Train Dataset"
      ],
      "metadata": {
        "id": "Ppj_xaRHxCbx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEZX3BBRwU_T"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "with open('train-v2.0 (1).json', 'r') as f:\n",
        "    squad_data = json.load(f)\n",
        "\n",
        "# Initialize the list to store transformed data\n",
        "train_questions_answers = []\n",
        "\n",
        "# Iterate through the SQuAD dataset\n",
        "for article in squad_data['data']:\n",
        "    for paragraph in article['paragraphs']:\n",
        "        context = paragraph['context']\n",
        "        for qa in paragraph['qas']:\n",
        "            question_id = qa['id']\n",
        "            is_impossible = qa.get('is_impossible', False)\n",
        "            question_text = qa['question']\n",
        "            answers = qa['answers'] if 'answers' in qa else []\n",
        "\n",
        "            # Create a dictionary for question-answer pair with context included\n",
        "            qa_pair = {\n",
        "                \"context\": context,\n",
        "                \"qas\": [\n",
        "                    {\n",
        "                        \"id\": question_id,\n",
        "                        \"is_impossible\": is_impossible,\n",
        "                        \"question\": question_text,\n",
        "                        \"answers\": answers\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "            # Append the question-answer pair to the list\n",
        "            train_questions_answers.append(qa_pair)\n",
        "\n",
        "# Save the transformed data into a JSON file\n",
        "output_file_path = 'train_questions_answers_with_context.json'\n",
        "with open(output_file_path, 'w') as outfile:\n",
        "    json.dump(train_questions_answers, outfile, indent=4)\n",
        "\n",
        "print(\"Data saved to:\", output_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "rsMz5B3QxLe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simpletransformers"
      ],
      "metadata": {
        "id": "KKoueF5NUMUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the entire dataset\n",
        "print(\"Loading dataset...\")\n",
        "with open(r\"train_questions_answers_with_context.json\", \"r\") as read_file:\n",
        "    data = json.load(read_file)\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "print(\"Splitting dataset into train and test sets...\")\n",
        "train_data, test_data = train_test_split(data, test_size=0.2)\n",
        "print(f\"Dataset split complete. Train set size: {len(train_data)}, Test set size: {len(test_data)}\")\n",
        "\n",
        "model_type = \"bert\"\n",
        "model_name = \"base-model\"\n",
        "\n",
        "\n",
        "# Define training arguments\n",
        "print(\"Defining training arguments...\")\n",
        "# Define training arguments\n",
        "train_args = {\n",
        "    \"reprocess_input_data\": True,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"use_cached_eval_features\": True,\n",
        "    \"output_dir\": f\"outputs/{model_type}\",\n",
        "    \"best_model_dir\": f\"outputs/{model_type}/best_model\",\n",
        "    \"evaluate_during_training\": True,\n",
        "    \"evaluate_during_training_steps\": 1000,\n",
        "    \"save_model_every_epoch\": False,\n",
        "    \"save_eval_checkpoints\": False,\n",
        "    \"n_best_size\": 3,\n",
        "    \"eval_batch_size\": 18,\n",
        "    \"eval_metrics\": ['exact_match'],\n",
        "    \"num_train_epochs\": 15,\n",
        "    \"early_stopping_patience\": 3,\n",
        "    \"evaluate_during_training_verbose\": True\n",
        "}\n",
        "print(\"Training arguments defined.\")\n",
        "\n",
        "# Initialize the model\n",
        "print(f\"Initializing the model with type: {model_type}, name: {model_name}...\")\n",
        "model = QuestionAnsweringModel(model_type, model_name, args=train_args)\n",
        "print(\"Model initialized.\")\n",
        "\n",
        "# Train the model on the new train set\n",
        "print(\"Starting model training...\")\n",
        "model.train_model(train_data, eval_data=test_data)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Save the best model directory as a zip file\n",
        "print(\"Saving the best model directory as a zip file...\")\n",
        "shutil.make_archive('final', 'zip', root_dir=os.path.join('outputs', model_type, 'best_model'))\n",
        "print(\"Model saved as 'final.zip'.\")\n"
      ],
      "metadata": {
        "id": "gttN5Fmgwcdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}